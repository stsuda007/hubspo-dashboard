import json
import gspread
import pandas as pd
import streamlit as st
import time
from datetime import datetime
from gspread.exceptions import APIError
from oauth2client.service_account import ServiceAccountCredentials

# --- èªè¨¼ ---
# Googleã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®èªè¨¼æƒ…å ±ã‚’å–å¾—ã—ã€èªè¨¼ã‚’è¡Œã†
# èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’ã‚¹ãƒ­ãƒ¼ã—ã€ã‚¢ãƒ—ãƒªã‚’åœæ­¢
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
try:
    credentials_dict = json.loads(st.secrets["GOOGLE_SERVICE_ACCOUNT"])
    creds = ServiceAccountCredentials.from_json_keyfile_dict(credentials_dict, scope)
    gc = gspread.authorize(creds)
except KeyError:
    st.error("Googleã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`st.secrets`ã«`GOOGLE_SERVICE_ACCOUNT`ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- å®šæ•° ---
SPREADSHEET_KEY = "1Ra_tPm2u5K4ikxacw1vdQqY_YQg-JekMsM-ZhaaVFKg"
DEALS_SHEET = "Deals"
STAGES_SHEET = "OtherParams"
USERS_SHEET = "Users"

# --- ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼†ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰ ---
# st.cache_dataã‚’ä½¿ã£ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚’300ç§’é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã€APIã‚³ãƒ¼ãƒ«ã‚’å‰Šæ¸›
@st.cache_data(ttl=300, show_spinner="Google Sheets ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
def load_data_with_retry(max_retries=3, delay=5):
    """
    Google Sheetsã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€APIåˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ã€‚

    Args:
        max_retries (int): å†è©¦è¡Œã®æœ€å¤§å›æ•°ã€‚
        delay (int): å†è©¦è¡Œã¾ã§ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰ã€‚

    Returns:
        tuple: Deals, Stages, Usersã®å„DataFrameã‚’è¿”ã™ã€‚å¤±æ•—ã—ãŸå ´åˆã¯ç©ºã®DataFrameã€‚
    """
    attempt = 0
    while attempt < max_retries:
        try:
            # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¨å„ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã‚’é–‹ã
            sh = gc.open_by_key(SPREADSHEET_KEY)
            deals_ws = sh.worksheet(DEALS_SHEET)
            stages_ws = sh.worksheet(STAGES_SHEET)
            users_ws = sh.worksheet(USERS_SHEET)

            # ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã¨ã—ã¦å–å¾—
            deals_data = pd.DataFrame(deals_ws.get_all_records())
            # OtherParamsã‚·ãƒ¼ãƒˆã¯æŒ‡å®šã—ãŸã‚»ãƒ«ç¯„å›²ã®ã¿ã‚’å–å¾—
            stages_data = pd.DataFrame(stages_ws.get("A2:B12"), columns=["Stage ID", "Stage Name"])
            users_data = pd.DataFrame(users_ws.get_all_records())

            return deals_data, stages_data, users_data

        except APIError as e:
            # APIåˆ¶é™ã‚¨ãƒ©ãƒ¼(429)ã®å ´åˆã¯å†è©¦è¡Œ
            if "429" in str(e):
                st.warning(f"APIåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚{delay}ç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™...ï¼ˆ{attempt + 1}/{max_retries}ï¼‰")
                time.sleep(delay)
                attempt += 1
            else:
                # ãã®ä»–ã®APIã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å‡¦ç†ã‚’ä¸­æ–­
                st.error(f"Google Sheets API ã‚¨ãƒ©ãƒ¼: {e}")
                break
        except Exception as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            break

    st.error("Google Sheetsã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å¾Œã»ã©å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
    return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

# --- ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•° ---
def process_and_merge_data(deals_df, stages_df, users_df):
    """
    ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€å¿…è¦ãªæƒ…å ±ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹ã€‚
    
    Args:
        deals_df (pd.DataFrame): Dealsã‚·ãƒ¼ãƒˆã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã€‚
        stages_df (pd.DataFrame): Stagesã‚·ãƒ¼ãƒˆã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã€‚
        users_df (pd.DataFrame): Usersã‚·ãƒ¼ãƒˆã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã€‚

    Returns:
        pd.DataFrame: å‡¦ç†ãƒ»ãƒãƒ¼ã‚¸æ¸ˆã¿ã®DataFrameã€‚
    """
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’ä½œæˆ
    users_df["Full Name"] = users_df["First Name"].fillna("") + " " + users_df["Last Name"].fillna("")
    users_df = users_df.rename(columns={"ID": "User ID"})
    
    # ãƒãƒ¼ã‚¸ã—ã‚„ã™ã„ã‚ˆã†ã«ã‚«ãƒ©ãƒ åã‚’å¤‰æ›´
    deals_df = deals_df.rename(columns={"Deal owner": "User ID", "Deal Stage": "Stage ID"})

    # ãƒ‡ãƒ¼ã‚¿å‹ã®å¤‰æ›ï¼ˆã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–ã—ã¦å‡¦ç†ï¼‰
    deals_df["User ID"] = pd.to_numeric(deals_df["User ID"], errors="coerce")
    deals_df["Stage ID"] = pd.to_numeric(deals_df["Stage ID"], errors="coerce")
    stages_df["Stage ID"] = pd.to_numeric(stages_df["Stage ID"], errors="coerce")
    
    # å—æ³¨é‡‘é¡ã‹ã‚‰é€šè²¨è¨˜å·ã‚„ã‚«ãƒ³ãƒã‚’é™¤å»ã—ã€æ•°å€¤ã«å¤‰æ›
    deals_df['å—æ³¨é‡‘é¡'] = deals_df['å—æ³¨é‡‘é¡'].astype(str).str.replace(r'[^\d]', '', regex=True)
    deals_df["å—æ³¨é‡‘é¡"] = pd.to_numeric(deals_df["å—æ³¨é‡‘é¡"], errors="coerce")
    
    # è¤‡æ•°DataFrameã‚’ãƒãƒ¼ã‚¸
    merged_df = deals_df.merge(users_df[["User ID", "Full Name"]], on="User ID", how="left")
    merged_df = merged_df.merge(stages_df, on="Stage ID", how="left")
    
    return merged_df

# --- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ¡ˆä»¶ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºé–¢æ•° ---
def display_pipeline_projects_table(df):
    """
    ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ¡ˆä»¶ã®ä¸€è¦§ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦è¡¨ç¤ºã™ã‚‹ã€‚
    
    Args:
        df (pd.DataFrame): å‡¦ç†æ¸ˆã¿ã®DataFrameã€‚
    """
    st.subheader("ğŸ“Š ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ¡ˆä»¶ä¸€è¦§")
    
    # å—æ³¨ç›®æ¨™æ—¥ã¾ãŸã¯ç´å“äºˆå®šæ—¥ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æ¡ˆä»¶ã®ã¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    df_pipeline = df[df['å—æ³¨ç›®æ¨™æ—¥'].notna() | df['ç´å“äºˆå®šæ—¥'].notna()].copy()
    
    if df_pipeline.empty:
        st.info("å—æ³¨ç›®æ¨™æ—¥ã¾ãŸã¯ç´å“äºˆå®šæ—¥ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æ¡ˆä»¶ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # è¡¨ç¤ºç”¨ã«ã‚«ãƒ©ãƒ åã‚’å¤‰æ›´
    display_df = df_pipeline.rename(columns={
        'Full Name': 'å–¶æ¥­æ‹…å½“è€…', 
        'Deal Name': 'æ¡ˆä»¶å'
    })
    
    # è¡¨ç¤ºã™ã‚‹ã‚«ãƒ©ãƒ ã‚’é¸æŠ
    cols_to_display = [
        'å–¶æ¥­æ‹…å½“è€…',
        'æ¡ˆä»¶å',
        'å—æ³¨ç›®æ¨™æ—¥',
        'ç´å“äºˆå®šæ—¥',
        'Stage Name',
        'è¦‹è¾¼å£²ä¸Šé¡'
    ]
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿ã‚’æŠ½å‡º
    display_df = display_df[cols_to_display]
    
    # æ—¥ä»˜å‹ã«å¤‰æ›
    for col in ['å—æ³¨ç›®æ¨™æ—¥', 'ç´å“äºˆå®šæ—¥']:
        display_df[col] = pd.to_datetime(display_df[col], errors='coerce').dt.strftime('%Y-%m-%d')
        
    # ã‚½ãƒ¼ãƒˆ
    display_df = display_df.sort_values(by=['å–¶æ¥­æ‹…å½“è€…', 'è¦‹è¾¼å£²ä¸Šé¡'], ascending=[True, False])
    # æ—¥ä»˜å‹ã«å¤‰æ›ï¼ˆã‚¨ãƒ©ãƒ¼å‡¦ç†ã‚’è¿½åŠ ï¼‰
    for col in ['å—æ³¨ç›®æ¨™æ—¥', 'ç´å“äºˆå®šæ—¥']:
        display_df[col] = pd.to_datetime(display_df[col], errors='coerce')  # Invalid values become NaT
        display_df[col] = display_df[col].fillna(pd.Timestamp('2020-01-01'))  # Optional: fill NaT with a default value

    # å–¶æ¥­æ‹…å½“è€…ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    grouped = display_df.groupby('å–¶æ¥­æ‹…å½“è€…')
    
    # å„å–¶æ¥­æ‹…å½“è€…ã®ãƒ‡ãƒ¼ã‚¿ã‚’å€‹åˆ¥ã«è¡¨ç¤º
    for name, group in grouped:
        st.subheader(f"å–¶æ¥­æ‹…å½“è€…: {name}")
        # ã‚½ãƒ¼ãƒˆ
        group = group.sort_values(by=['å—æ³¨ç›®æ¨™æ—¥'], ascending=[True, False])
        # Streamlitã§ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¡¨ç¤º
        st.dataframe(group, use_container_width=True)

# --- ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œéƒ¨åˆ† ---
def main():
    """
    Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    st.title("HubSpot Deals ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")

    # ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ã¨ãƒã‚§ãƒƒã‚¯
    deals_df, stages_df, users_df = load_data_with_retry()
    if deals_df.empty or stages_df.empty or users_df.empty:
        st.error("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸãŸã‚ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åœæ­¢ã—ã¾ã™ã€‚")
        st.stop()

    # ãƒ‡ãƒ¼ã‚¿ã®åŠ å·¥ã¨ãƒãƒ¼ã‚¸
    merged_df = process_and_merge_data(deals_df, stages_df, users_df)

    # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
    display_pipeline_projects_table(merged_df)

if __name__ == "__main__":
    main()
