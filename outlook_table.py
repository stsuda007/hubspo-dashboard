import json
import gspread
import pandas as pd
import streamlit as st
import time
from datetime import datetime, timedelta
from gspread.exceptions import APIError
from oauth2client.service_account import ServiceAccountCredentials

# --- Streamlitãƒšãƒ¼ã‚¸ã®åŸºæœ¬è¨­å®š ---
st.set_page_config(
    page_title="Hubspot Dashboard",
    page_icon="ğŸ§Š",
    layout="wide",  # streamlitãŒç”»é¢ã„ã£ã±ã„ã«ä½¿ã†
    initial_sidebar_state="expanded",
)

# --- èªè¨¼ ---
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
try:
    credentials_dict = json.loads(st.secrets["GOOGLE_SERVICE_ACCOUNT"])
    creds = ServiceAccountCredentials.from_json_keyfile_dict(credentials_dict, scope)
    gc = gspread.authorize(creds)
except KeyError:
    st.error("Googleã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`st.secrets`ã«`GOOGLE_SERVICE_ACCOUNT`ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚­ãƒ¼ã‚’ã€st.secretsã‹ã‚‰èª­ã¿è¾¼ã‚€ã‚ˆã†ã«å¤‰æ›´
try:
    SPREADSHEET_KEY = st.secrets["SPREADSHEET_KEY"]
except KeyError:
    st.error("Google Sheetsã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`st.secrets`ã«`SPREADSHEET_KEY`ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- å®šæ•° ---
DEALS_SHEET = "Deals"
STAGES_SHEET = "OtherParams"
USERS_SHEET = "Users"

# --- helpers ---
def _norm(x):
    if pd.isna(x):
        return ""
    return str(x).strip().lower()
def strike_text(s: str) -> str:
    """U+0336 ã‚’ä½¿ã£ãŸæ‰“æ¶ˆç·šï¼ˆDataFrameã§ã‚‚å´©ã‚Œã«ãã„ï¼‰"""
    if pd.isna(s):
        return s
    s = str(s)
    return "".join(ch + "\u0336" for ch in s) + "\u0336"
def is_lost_row(row: pd.Series) -> bool:
    """å¤±æ³¨ã‚’å¤šé¢çš„ã«åˆ¤å®šï¼ˆåˆ—åã¯å­˜åœ¨ã™ã‚Œã°ä½¿ã† / ç„¡ã‘ã‚Œã°ç„¡è¦–ï¼‰"""
    stage_name = _norm(row.get("Stage Name"))
    status_jp  = _norm(row.get("å—æ³¨/å¤±æ³¨"))
    deal_stat  = _norm(row.get("Deal Status"))
    lost_date  = row.get("å¤±æ³¨æ—¥")

    return (
        ("å¤±æ³¨" in stage_name) or
        ("lost" in stage_name) or
        ("å¤±æ³¨" in status_jp) or
        ("closed lost" in deal_stat) or
        (pd.notna(lost_date) and str(lost_date).strip() != "")
    )
def apply_strike_style(df: pd.DataFrame):
    """is_lost==True ã®è¡Œã‚’ã¾ã‚‹ã”ã¨æ‰“æ¶ˆã—ç·šã«ã™ã‚‹"""
    def strike_style(row):
        if bool(row.get('is_lost', False)):
            return ['text-decoration: line-through'] * len(row)
        return [''] * len(row)
    return df.style.apply(strike_style, axis=1)
def apply_dim_style(df: pd.DataFrame, mode: str = "both",
                    text_gray: str = "#6b7280",   # Tailwind: gray-500
                    bg_gray: str = "#f3f4f6"):    # Tailwind: gray-100
    """
    is_lost==True ã®è¡Œã‚’ã‚°ãƒ¬ãƒ¼ã‚¢ã‚¦ãƒˆã€‚
    mode: "text"ï¼ˆæ–‡å­—ã ã‘ï¼‰, "bg"ï¼ˆèƒŒæ™¯ã ã‘ï¼‰, "both"ï¼ˆä¸¡æ–¹ï¼‰
    """
    def style_row(row):
        if bool(row.get('is_lost', False)):
            styles = []
            for _ in row.index:
                parts = []
                if mode in ("text", "both"):
                    parts.append(f"color: {text_gray}")
                if mode in ("bg", "both"):
                    parts.append(f"background-color: {bg_gray}")
                styles.append("; ".join(parts))
            return styles
        return [""] * len(row)
    return df.style.apply(style_row, axis=1)


# --- ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼†ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰ ---
@st.cache_data(ttl=300, show_spinner="Google Sheets ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
def load_data_with_retry(max_retries=3, delay=5):
    attempt = 0
    while attempt < max_retries:
        try:
            sh = gc.open_by_key(SPREADSHEET_KEY)
            deals_ws = sh.worksheet(DEALS_SHEET)
            stages_ws = sh.worksheet(STAGES_SHEET)
            users_ws = sh.worksheet(USERS_SHEET)

            deals_data = pd.DataFrame(deals_ws.get_all_records())
            stages_data = pd.DataFrame(stages_ws.get("A2:B12"), columns=["Stage ID", "Stage Name"])
            users_data = pd.DataFrame(users_ws.get_all_records())

            return deals_data, stages_data, users_data

        except APIError as e:
            if "429" in str(e):
                st.warning(f"APIåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚{delay}ç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™...ï¼ˆ{attempt + 1}/{max_retries}ï¼‰")
                time.sleep(delay)
                attempt += 1
            else:
                st.error(f"Google Sheets API ã‚¨ãƒ©ãƒ¼: {e}")
                break
        except Exception as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            break

    st.error("Google Sheetsã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å¾Œã»ã©å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
    return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

# --- ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•° ---
def process_and_merge_data(deals_df, stages_df, users_df):
    users_df["Full Name"] = users_df["Last Name"].fillna("") + " " + users_df["First Name"].fillna("")
    users_df = users_df.rename(columns={"ID": "User ID"})
    
    deals_df = deals_df.rename(columns={"Deal owner": "User ID", "Deal Stage": "Stage ID"})

    deals_df["User ID"] = pd.to_numeric(deals_df["User ID"], errors="coerce")
    deals_df["Stage ID"] = pd.to_numeric(deals_df["Stage ID"], errors="coerce")
    stages_df["Stage ID"] = pd.to_numeric(stages_df["Stage ID"], errors="coerce")
    
    # é‡‘é¡ã‚«ãƒ©ãƒ ã®å‰å‡¦ç†ã‚’å¼·åŒ–
    deals_df['å—æ³¨é‡‘é¡'] = deals_df['å—æ³¨é‡‘é¡'].astype(str).str.replace(r'[^\d]', '', regex=True)
    deals_df["å—æ³¨é‡‘é¡"] = pd.to_numeric(deals_df["å—æ³¨é‡‘é¡"], errors="coerce")
    deals_df['è¦‹è¾¼å£²ä¸Šé¡'] = deals_df['è¦‹è¾¼å£²ä¸Šé¡'].astype(str).str.replace(r'[^\d]', '', regex=True)
    deals_df["è¦‹è¾¼å£²ä¸Šé¡"] = pd.to_numeric(deals_df["è¦‹è¾¼å£²ä¸Šé¡"], errors="coerce")
    
    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•°å†…ã§ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®åˆ—ã‚’ç”Ÿæˆ
    deals_df['è¦‹è¾¼å£²ä¸Šé¡ï¼ˆå††ï¼‰'] = deals_df['è¦‹è¾¼å£²ä¸Šé¡'].apply(lambda x: f"ï¿¥{x:,.0f}" if pd.notna(x) else "")
    deals_df['å—æ³¨é‡‘é¡ï¼ˆå††ï¼‰'] = deals_df['å—æ³¨é‡‘é¡'].apply(lambda x: f"ï¿¥{x:,.0f}" if pd.notna(x) else "")
    
    merged_df = deals_df.merge(users_df[["User ID", "Full Name"]], on="User ID", how="left")
    merged_df = merged_df.merge(stages_df, on="Stage ID", how="left")

    # å¤±æ³¨åˆ¤å®šãƒ•ãƒ©ã‚°
    merged_df["is_lost"] = merged_df.apply(is_lost_row, axis=1)
    
    return merged_df

# --- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ¡ˆä»¶ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºé–¢æ•° ---
def display_pipeline_projects_table(df):
    """
    ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ¡ˆä»¶ã®ä¸€è¦§ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦è¡¨ç¤ºã™ã‚‹ã€‚
    """
    # æ—¥ä»˜åˆ—ã‚’datetimeå‹ã«å¤‰æ›
    df['å—æ³¨ç›®æ¨™æ—¥_dt'] = pd.to_datetime(df['å—æ³¨ç›®æ¨™æ—¥'], errors='coerce')
    df['ç´å“äºˆå®šæ—¥_dt'] = pd.to_datetime(df['ç´å“äºˆå®šæ—¥'], errors='coerce')

    # ç¾åœ¨ã®æ—¥ä»˜ã‚’å–å¾—
    today = datetime.now()
    first_day_of_current_month = today.replace(day=1)

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: å—æ³¨ç›®æ¨™æ—¥ã¾ãŸã¯ç´å“äºˆå®šæ—¥ãŒä»Šæœˆä»¥é™ã®æ¡ˆä»¶
    df_pipeline = df[(df['å—æ³¨ç›®æ¨™æ—¥_dt'] >= first_day_of_current_month) | (df['ç´å“äºˆå®šæ—¥_dt'] >= first_day_of_current_month)].copy()
    if df_pipeline.empty:
        st.info("ä»Šæœˆä»¥é™ã®å—æ³¨ç›®æ¨™æ—¥ã¾ãŸã¯ç´å“äºˆå®šæ—¥ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æ¡ˆä»¶ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # è¡¨ç¤ºç”¨ã«ã‚«ãƒ©ãƒ åã‚’å¤‰æ›´ã—ã€is_lost ã‚’å¼•ãç¶™ã
    display_df = (
        df_pipeline
        .rename(columns={
            'Full Name': 'å–¶æ¥­æ‹…å½“è€…',
            'Deal Name': 'æ¡ˆä»¶å',
            'Stage Name': 'ãƒ•ã‚§ãƒ¼ã‚º'
        })
        .assign(is_lost=df_pipeline['is_lost'].values)
    )

    # æ‰“æ¶ˆç·šä»˜ãã®è¡¨ç¤ºç”¨æ¡ˆä»¶å
    # display_df['æ¡ˆä»¶å_è¡¨ç¤º'] = display_df.apply(
    #    lambda r: strike_text(r['æ¡ˆä»¶å']) if r['is_lost'] else r['æ¡ˆä»¶å'],
    #    axis=1
    # )
    # ç½®ãæ›ãˆï¼ˆå¤±æ³¨ã®ã¨ãã‚‚ãã®ã¾ã¾æ–‡å­—ã‚’è¦‹ã›ã‚‹ï¼‰
    display_df['æ¡ˆä»¶å_è¡¨ç¤º'] = display_df['æ¡ˆä»¶å']

    # `cols_to_display`ã§åˆ—ã®é †åºã‚’çµ±ä¸€ï¼ˆis_lost ã¯å†…éƒ¨ç”¨ã«ä¿æŒã€è¡¨ã§ã¯éè¡¨ç¤ºï¼‰
    cols_to_display = [
        'å–¶æ¥­æ‹…å½“è€…',
        'æ¡ˆä»¶å_è¡¨ç¤º',
        'å—æ³¨ç›®æ¨™æ—¥_dt',
        'ç´å“äºˆå®šæ—¥_dt',
        'è¦‹è¾¼å£²ä¸Šé¡ï¼ˆå††ï¼‰',
        'å—æ³¨é‡‘é¡ï¼ˆå††ï¼‰',
        'ãƒ•ã‚§ãƒ¼ã‚º',
        'è¦‹è¾¼å£²ä¸Šé¡',    # é›†è¨ˆç”¨ï¼ˆéè¡¨ç¤ºï¼‰
        'å—æ³¨é‡‘é¡',      # é›†è¨ˆç”¨ï¼ˆéè¡¨ç¤ºï¼‰
        'is_lost'        # é›†è¨ˆç”¨ï¼ˆéè¡¨ç¤ºï¼‰
    ]
    display_df = display_df[cols_to_display]

    # --- æœˆã”ã¨ã®è¡¨ç¤º ---
    st.subheader("æœˆåˆ¥ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
    next_month = (today.replace(day=1) + timedelta(days=32)).replace(day=1)
    two_months_later = (today.replace(day=1) + timedelta(days=62)).replace(day=1)
    three_months_later = (today.replace(day=1) + timedelta(days=93)).replace(day=1)

    def get_month_group(date):
        if pd.isna(date):
            return "ãã®ä»–"
        if date.year == today.year and date.month == today.month:
            return f"{today.month}æœˆ"
        elif date.year == next_month.year and date.month == next_month.month:
            return f"{next_month.month}æœˆ"
        elif date.year == two_months_later.year and date.month == two_months_later.month:
            return f"{two_months_later.month}æœˆ"
        elif date.year == three_months_later.year and date.month == three_months_later.month:
            return f"{three_months_later.month}æœˆ"
        else:
            return "ãã®ä»–"

    display_df['Grouping Month'] = display_df['å—æ³¨ç›®æ¨™æ—¥_dt'].apply(get_month_group)
    grouped_by_month = display_df.groupby('Grouping Month')
    current_month_name = f"{today.month}æœˆ"
    next_month_name = f"{next_month.month}æœˆ"
    two_months_later_name = f"{two_months_later.month}æœˆ"
    three_months_later_name = f"{three_months_later.month}æœˆ"
    custom_order = [current_month_name, next_month_name, two_months_later_name, three_months_later_name, "ãã®ä»–"]
    sorted_groups = sorted(grouped_by_month, key=lambda x: custom_order.index(x[0]) if x[0] in custom_order else 99)

    # è¡¨ç¤ºã™ã‚‹åˆ—ã®é †åºã‚’å®šç¾©ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºæ™‚ã« is_lost ã¯éè¡¨ç¤ºï¼‰
    month_table_order = ('å–¶æ¥­æ‹…å½“è€…', 'æ¡ˆä»¶å_è¡¨ç¤º', 'å—æ³¨ç›®æ¨™æ—¥_dt', 'ç´å“äºˆå®šæ—¥_dt', 'è¦‹è¾¼å£²ä¸Šé¡ï¼ˆå††ï¼‰', 'å—æ³¨é‡‘é¡ï¼ˆå††ï¼‰', 'ãƒ•ã‚§ãƒ¼ã‚º')

    for name, group2 in sorted_groups:
        total_outlook2 = group2.loc[~group2['is_lost'], 'è¦‹è¾¼å£²ä¸Šé¡'].sum()
        with st.expander(f"{name} ãƒ¼ å£²ä¸Šè¦‹è¾¼é¡: {total_outlook2:,.0f}"):
            view_df = (
                group2
                .drop(columns=['Grouping Month'])
                .sort_values(by=['å—æ³¨ç›®æ¨™æ—¥_dt','is_lost'], ascending=[True,True], na_position='last')
                # åˆ—é †ã¯ã“ã“ã§æƒãˆã‚‹ï¼ˆcolumn_order ã‚’ä½¿ã‚ãªã„æƒ³å®šï¼‰
                [['å–¶æ¥­æ‹…å½“è€…','æ¡ˆä»¶å_è¡¨ç¤º','å—æ³¨ç›®æ¨™æ—¥_dt','ç´å“äºˆå®šæ—¥_dt','è¦‹è¾¼å£²ä¸Šé¡ï¼ˆå††ï¼‰','å—æ³¨é‡‘é¡ï¼ˆå††ï¼‰','ãƒ•ã‚§ãƒ¼ã‚º']]
            )
            #styled = apply_strike_text(view_df)
            #styled = apply_dim_style(view_df, mode="both")

            st.dataframe(
                styled,
                column_config={
                    "æ¡ˆä»¶å_è¡¨ç¤º": st.column_config.TextColumn("æ¡ˆä»¶å"),
                    "è¦‹è¾¼å£²ä¸Šé¡ï¼ˆå††ï¼‰": st.column_config.TextColumn("è¦‹è¾¼å£²ä¸Šé¡", help="æ¡ˆä»¶ã®äºˆæƒ³å£²ä¸Šé‡‘é¡"),
                    "å—æ³¨é‡‘é¡ï¼ˆå††ï¼‰": st.column_config.TextColumn("å—æ³¨é‡‘é¡", help="å—æ³¨ãŒç¢ºå®šã—ãŸé‡‘é¡"),
                    "å—æ³¨ç›®æ¨™æ—¥_dt": st.column_config.DateColumn("å—æ³¨ç›®æ¨™æ—¥", format="MM/DD"),
                    "ç´å“äºˆå®šæ—¥_dt": st.column_config.DateColumn("ç´å“äºˆå®šæ—¥", format="MM/DD"),
                },
                hide_index=True,
                use_container_width=True,
                height=300,
            )
            total_sum = group2.loc[~group2['is_lost'], 'å—æ³¨é‡‘é¡'].sum()
            total_outlook = group2.loc[~group2['is_lost'], 'è¦‹è¾¼å£²ä¸Šé¡'].sum()
            st.markdown(f"**åˆè¨ˆå—æ³¨é‡‘é¡: {total_sum:,.0f}ã€€åˆè¨ˆå£²ä¸Šè¦‹è¾¼é¡: {total_outlook:,.0f}**")


    # --- æ‹…å½“è€…ã”ã¨ã®è¡¨ç¤º ---
    st.subheader("å–¶æ¥­æ‹…å½“è€…åˆ¥ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
    
    # æ‹…å½“è€…ã”ã¨ã®ã‚½ãƒ¼ãƒˆã¨ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    sorted_by_user_df = display_df.sort_values(
        by=['å–¶æ¥­æ‹…å½“è€…', 'å—æ³¨ç›®æ¨™æ—¥_dt'],
        ascending=[True, True],
        na_position='last'
    )
    grouped_by_user = sorted_by_user_df.groupby('å–¶æ¥­æ‹…å½“è€…')

    # ãƒ†ãƒ¼ãƒ–ãƒ«åˆ—é †ï¼ˆæ‹…å½“è€…åˆ¥ï¼‰
    display_columns = ('å–¶æ¥­æ‹…å½“è€…', 'æ¡ˆä»¶å_è¡¨ç¤º', 'å—æ³¨ç›®æ¨™æ—¥_dt', 'ç´å“äºˆå®šæ—¥_dt', 'è¦‹è¾¼å£²ä¸Šé¡ï¼ˆå††ï¼‰', 'å—æ³¨é‡‘é¡ï¼ˆå††ï¼‰', 'ãƒ•ã‚§ãƒ¼ã‚º')

    # å„æ‹…å½“è€…ã®ãƒ‡ãƒ¼ã‚¿ã‚’å€‹åˆ¥ã«è¡¨ç¤º
    for name, group_df in grouped_by_user:
        with st.expander(f"{name} ãƒ¼ æ¡ˆä»¶æ•°:{group_df.shape[0]}"):
            view_df = (
                group_df
                .drop(columns=['Grouping Month'])
                .sort_values(by=['å—æ³¨ç›®æ¨™æ—¥_dt','is_lost'], ascending=[True,True], na_position='last')
                [['å–¶æ¥­æ‹…å½“è€…','æ¡ˆä»¶å_è¡¨ç¤º','å—æ³¨ç›®æ¨™æ—¥_dt','ç´å“äºˆå®šæ—¥_dt','è¦‹è¾¼å£²ä¸Šé¡ï¼ˆå††ï¼‰','å—æ³¨é‡‘é¡ï¼ˆå††ï¼‰','ãƒ•ã‚§ãƒ¼ã‚º']]
            )
            # styled = apply_strike_style(view_df)
            styled = apply_dim_style(view_df, mode = "bg")

            st.dataframe(
                styled,
                column_config={
                    "æ¡ˆä»¶å_è¡¨ç¤º": st.column_config.TextColumn("æ¡ˆä»¶å"),
                    "è¦‹è¾¼å£²ä¸Šé¡ï¼ˆå††ï¼‰": st.column_config.TextColumn("è¦‹è¾¼å£²ä¸Šé¡"),
                    "å—æ³¨é‡‘é¡ï¼ˆå††ï¼‰": st.column_config.TextColumn("å—æ³¨é‡‘é¡"),
                    "å—æ³¨ç›®æ¨™æ—¥_dt": st.column_config.DateColumn("å—æ³¨ç›®æ¨™æ—¥", format="MM/DD"),
                    "ç´å“äºˆå®šæ—¥_dt": st.column_config.DateColumn("ç´å“äºˆå®šæ—¥", format="MM/DD"),
                },
                use_container_width=True,
                height=300,
                hide_index=True,
            )

            total_sum = group_df.loc[~group_df['is_lost'], 'å—æ³¨é‡‘é¡'].sum()
            total_outlook = group_df.loc[~group_df['is_lost'], 'è¦‹è¾¼å£²ä¸Šé¡'].sum()
            st.markdown(f"**åˆè¨ˆå—æ³¨é‡‘é¡: {total_sum:,.0f}ã€€åˆè¨ˆå£²ä¸Šè¦‹è¾¼é¡: {total_outlook:,.0f}**")


# --- ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œéƒ¨åˆ† ---
def main():
    st.markdown(f'<h2 style="color:#444444;font-size:24px;">{"å—æ³¨ç›®æ¨™ã®ã‚ã‚‹æ¡ˆä»¶ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"}</h2>', unsafe_allow_html=True)
    deals_df, stages_df, users_df = load_data_with_retry()
    if deals_df.empty or stages_df.empty or users_df.empty:
        st.error("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸãŸã‚ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åœæ­¢ã—ã¾ã™ã€‚")
        st.stop()
    merged_df = process_and_merge_data(deals_df, stages_df, users_df)
    display_pipeline_projects_table(merged_df)

if __name__ == "__main__":
    main()
